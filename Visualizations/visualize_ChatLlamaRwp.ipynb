{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc0e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_json(file_path): # Load JSON Data\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                return json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"JSON decode error â€” file is empty or malformed.\")\n",
    "                return {}\n",
    "    else:\n",
    "        return {}  # Return empty dict if file doesn't exist\n",
    "\n",
    "\n",
    "def update_json(data, key, value): # Modify/Add Data\n",
    "    data[key] = value\n",
    "    return data\n",
    "\n",
    "def save_json(file_path, data): # Save JSON Data\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c232725a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a chart with the interpretation selections of Chat GPT, Llama and the Rwp minimization\n",
    "#includes synthesis, interpretation info, balance score, Rwp, and likelihoods for each LLM\n",
    "\n",
    "def find_all_choices(json_file):\n",
    "    sample_choice = {}\n",
    "    for sample in json_file:\n",
    "        llama_probs = []\n",
    "        chat_probs = []\n",
    "        rwp_vals = []\n",
    "        sample_choice[sample] = {}\n",
    "        for interpret, info in json_file[sample].items():\n",
    "            if interpret == \"Synth_Conditions\":\n",
    "                continue\n",
    "            try:\n",
    "                llama_score = info[\"unnormalized_posterior_llama\"]\n",
    "                chat_score = info[\"unnormalized_posterior\"]\n",
    "                rwp = info[\"rwp\"]\n",
    "                if all(isinstance(x, (int, float)) for x in [llama_score, chat_score, rwp]):\n",
    "                    llama_probs.append((interpret, llama_score))\n",
    "                    chat_probs.append((interpret, chat_score))\n",
    "                    rwp_vals.append((interpret, rwp))\n",
    "            except KeyError as e:\n",
    "                print(f\"Missing key in sample '{sample}', interpretation '{interpret}': {e}\")\n",
    "        \n",
    "        sample_choice[sample][\"llama_choice\"] = max(llama_probs, key=lambda x: x[1])[0] if llama_probs else None\n",
    "        sample_choice[sample][\"chat_choice\"] = max(chat_probs, key=lambda x: x[1])[0] if chat_probs else None\n",
    "        sample_choice[sample][\"rwp_choice\"] = min(rwp_vals, key=lambda x: x[1])[0] if rwp_vals else None\n",
    "    \n",
    "    return sample_choice\n",
    "file_path = \"Data/prompt3/interpretations_llm_v1_llama1.json\"\n",
    "json_file = load_json(file_path)\n",
    "find_all_choices(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "990f6886",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TRI_105'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m     rwp_row[\u001b[33m\"\u001b[39m\u001b[33mLlama Likelihood\u001b[39m\u001b[33m\"\u001b[39m] = json_file[sample_id][rwp_choice][\u001b[33m\"\u001b[39m\u001b[33mLLM_interpretation_likelihood\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     52\u001b[39m sample_choice = find_all_choices(json_file)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m rows = \u001b[43massemble_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_choice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Create DataFrame\u001b[39;00m\n\u001b[32m     55\u001b[39m df = pd.DataFrame(rows)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36massemble_df\u001b[39m\u001b[34m(sample_id, sample_choice, json_file)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# assign Chat GPT row values\u001b[39;00m\n\u001b[32m     20\u001b[39m chat_row = {}\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m chat_choice = \u001b[43msample_choice\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_id\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mchat_choice\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     22\u001b[39m chat_row[\u001b[33m\"\u001b[39m\u001b[33mSelection Method\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mChat GPT\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m chat_row[\u001b[33m\"\u001b[39m\u001b[33mChoice\u001b[39m\u001b[33m\"\u001b[39m] = chat_choice\n",
      "\u001b[31mKeyError\u001b[39m: 'TRI_105'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"Data/prompt3/interpretations_llm_v1_llama1.json\"\n",
    "json_file = load_json(file_path)\n",
    "sample_id = \"TRI_105\"  # sample name\n",
    "\n",
    "def format_predicted_phases(json_file, sample_id, chat_choice):\n",
    "    phases = json_file[sample_id][chat_choice][\"phases\"]\n",
    "    wfs = json_file[sample_id][chat_choice][\"weight_fraction\"]\n",
    "    phase_list = \"\"\n",
    "    for i in range(len(phases)):\n",
    "        if not phase_list:\n",
    "            phase_list += f\"{phases[i]} {wfs[i]}\"\n",
    "        else: phase_list += f\", {phases[i]} {wfs[i]}\"\n",
    "    return phase_list\n",
    "\n",
    "# Helper function to extract values safely\n",
    "def assemble_df(sample_id, sample_choice, json_file):\n",
    "    rows = []\n",
    "    # assign Chat GPT row values\n",
    "    chat_row = {}\n",
    "    chat_choice = sample_choice[sample_id][\"chat_choice\"]\n",
    "    chat_row[\"Selection Method\"] = \"Chat GPT\"\n",
    "    chat_row[\"Choice\"] = chat_choice\n",
    "    chat_row[\"Predicted Phases\"] = format_predicted_phases(json_file, sample_id, chat_choice)\n",
    "    chat_row[\"Balance Score\"] = json_file[sample_id][chat_choice][\"balance_score\"]\n",
    "    chat_row[\"Rwp\"] = json_file[sample_id][chat_choice][\"rwp\"]\n",
    "    chat_row[\"Chat GPT Likelihood\"] = json_file[sample_id][chat_choice][\"LLM_interpretation_likelihood\"]\n",
    "    chat_row[\"Llama Likelihood\"] = json_file[sample_id][chat_choice][\"LLM_interpretation_likelihood\"]\n",
    "\n",
    "    #assign Llama row values\n",
    "    llama_row = {}\n",
    "    llama_choice = sample_choice[sample_id][\"llama_choice\"]\n",
    "    llama_row[\"Selection Method\"] = \"Llama\"\n",
    "    llama_row[\"Choice\"] = llama_choice\n",
    "    llama_row[\"Predicted Phases\"] = format_predicted_phases(json_file, sample_id, llama_choice)\n",
    "    llama_row[\"Balance Score\"] = json_file[sample_id][llama_choice][\"balance_score\"]\n",
    "    llama_row[\"Rwp\"] = json_file[sample_id][llama_choice][\"rwp\"]\n",
    "    llama_row[\"Chat GPT Likelihood\"] = json_file[sample_id][llama_choice][\"LLM_interpretation_likelihood\"]\n",
    "    llama_row[\"Llama Likelihood\"] = json_file[sample_id][llama_choice][\"LLM_interpretation_likelihood\"]\n",
    "\n",
    "    #assign lowest Rwp values\n",
    "    rwp_row = {}\n",
    "    rwp_choice = sample_choice[sample_id][\"rwp_choice\"]\n",
    "    rwp_row[\"Selection Method\"] = \"Lowest Rwp\"\n",
    "    rwp_row[\"Choice\"] = rwp_choice\n",
    "    rwp_row[\"Predicted Phases\"] = format_predicted_phases(json_file, sample_id, rwp_choice)\n",
    "    rwp_row[\"Balance Score\"] = json_file[sample_id][rwp_choice][\"balance_score\"]\n",
    "    rwp_row[\"Rwp\"] = json_file[sample_id][rwp_choice][\"rwp\"]\n",
    "    rwp_row[\"Chat GPT Likelihood\"] = json_file[sample_id][rwp_choice][\"LLM_interpretation_likelihood\"]\n",
    "    rwp_row[\"Llama Likelihood\"] = json_file[sample_id][rwp_choice][\"LLM_interpretation_likelihood\"]\n",
    "\n",
    "sample_choice = find_all_choices(json_file)\n",
    "rows = assemble_df(sample_id, sample_choice, json_file)\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "df = df[[\"Selection Method\" \"Choice\", \"Predicted Phases\", \"Balance Score\", \"Rwp\", \"Chat GPT Likelihood\", \"Llama Likelihood\"]]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bbff7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
